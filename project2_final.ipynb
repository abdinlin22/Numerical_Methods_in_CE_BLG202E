{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLG 202E - PROJECT 2 - NAZRIN ABDINLI - 150220925**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part 1 - Data Loading*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             author  posted_on  rating  \\\n",
      "0       Alantae of Chesterfeild, MI 2016-11-22       1   \n",
      "1          Vera of Philadelphia, PA 2016-11-19       1   \n",
      "2       Sarah of Rancho Cordova, CA 2016-11-17       1   \n",
      "3          Dennis of Manchester, NH 2016-11-16       1   \n",
      "4              Ryan of Bellevue, WA 2016-11-14       1   \n",
      "...                             ...        ...     ...   \n",
      "5196           Paul of Martinez, CA 2009-01-04       0   \n",
      "5197  Adelaide of Northwildwood, NJ 2009-01-03       0   \n",
      "5198       Michelle of Richmond, CA 2009-01-03       0   \n",
      "5199       Jesse of Newburyport, MA 2009-01-02       0   \n",
      "5200   Winston of Port St Lucie, FL 2009-01-01       0   \n",
      "\n",
      "                                                   text  \\\n",
      "0     I used to love Comcast. Until all these consta...   \n",
      "1     I'm so over Comcast! The worst internet provid...   \n",
      "2     If I could give them a negative star or no sta...   \n",
      "3     I've had the worst experiences so far since in...   \n",
      "4     Check your contract when you sign up for Comca...   \n",
      "...                                                 ...   \n",
      "5196  Cable TV service in my area has had intermitte...   \n",
      "5197  On August 16, 2008, I ordered the Triple Play....   \n",
      "5198  I called to disconnect my services and she sai...   \n",
      "5199  I had told Comcast that I no longer wanted my ...   \n",
      "5200  I live in an HUD Senior Citizen apartment buil...   \n",
      "\n",
      "                                         processed_text  \n",
      "0     use love comcast constant updat internet cabl ...  \n",
      "1     comcast worst internet provid take onlin class...  \n",
      "2     could give neg star star review would never wo...  \n",
      "3     worst experi far sinc instal noth problem two ...  \n",
      "4     check contract sign comcast advertis offer mat...  \n",
      "...                                                 ...  \n",
      "5196  cabl tv servic area intermitt poor pictur qual...  \n",
      "5197  august 16 2008 order tripl play rebat 225 doll...  \n",
      "5198  call disconnect servic said could balanc ask b...  \n",
      "5199  told comcast longer want bill paid directli ba...  \n",
      "5200  live hud senior citizen apart build manag cath...  \n",
      "\n",
      "[5058 rows x 5 columns]\n",
      "0       use love comcast constant updat internet cabl ...\n",
      "1       comcast worst internet provid take onlin class...\n",
      "2       could give neg star star review would never wo...\n",
      "3       worst experi far sinc instal noth problem two ...\n",
      "4       check contract sign comcast advertis offer mat...\n",
      "                              ...                        \n",
      "5196    cabl tv servic area intermitt poor pictur qual...\n",
      "5197    august 16 2008 order tripl play rebat 225 doll...\n",
      "5198    call disconnect servic said could balanc ask b...\n",
      "5199    told comcast longer want bill paid directli ba...\n",
      "5200    live hud senior citizen apart build manag cath...\n",
      "Name: processed_text, Length: 5058, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#loading the dataset\n",
    "df = pd.read_csv(\"comcast_consumeraffairs_complaints.csv\")\n",
    "\n",
    "df['posted_on'] = pd.to_datetime(df['posted_on'])                   #filtering complaints from 2009 onwards\n",
    "df_filter_2009 = df[df['posted_on'].dt.year >= 2009].copy()     \n",
    "df_filter_2009.dropna(subset=['text'], inplace=True)                #removing rows with missing complaint details  \n",
    "\n",
    "#text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "port_stem = PorterStemmer()\n",
    "wn_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())                                    #tokenization and lowercasing             \n",
    "    tokens = [token for token in tokens if token.isalnum()]                 #removing punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words]         #removing stopwords\n",
    "    tokens = [port_stem.stem(token) for token in tokens]                    #stemming\n",
    "    tokens = [wn_lemmatizer.lemmatize(token) for token in tokens]           #lemmatization\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df_filter_2009['processed_text'] = df_filter_2009['text'].apply(preprocess_text)\n",
    "\n",
    "print(df_filter_2009)\n",
    "print(df_filter_2009['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part 2 - Creating Term-by-Document Matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9452)\t0.12653350694307097\n",
      "  (0, 3901)\t0.0588287526489019\n",
      "  (0, 6434)\t0.13523351221802102\n",
      "  (0, 9416)\t0.1921738012832514\n",
      "  (0, 8287)\t0.11753315739347929\n",
      "  (0, 8480)\t0.11827174352068655\n",
      "  (0, 8546)\t0.17824454642837595\n",
      "  (0, 6852)\t0.1301991481420399\n",
      "  (0, 241)\t0.12739419833561247\n",
      "  (0, 909)\t0.11471980074942563\n",
      "  (0, 5526)\t0.32831412777811037\n",
      "  (0, 7866)\t0.2370881406641892\n",
      "  (0, 9364)\t0.1611811768578798\n",
      "  (0, 3181)\t0.13406483051292015\n",
      "  (0, 6423)\t0.13898244699133835\n",
      "  (0, 2676)\t0.1504094341214357\n",
      "  (0, 9394)\t0.07764296612606345\n",
      "  (0, 3362)\t0.08400328259544916\n",
      "  (0, 1927)\t0.11609937348033708\n",
      "  (0, 2563)\t0.07278968743523499\n",
      "  (0, 7867)\t0.3312338469851849\n",
      "  (0, 5846)\t0.15123694403354335\n",
      "  (0, 5227)\t0.14712531742347082\n",
      "  (0, 2416)\t0.4978095237967047\n",
      "  (0, 1776)\t0.07306376743997113\n",
      "  :\t:\n",
      "  (5057, 1091)\t0.10973287265876155\n",
      "  (5057, 5223)\t0.11335891272829174\n",
      "  (5057, 976)\t0.09707878127914688\n",
      "  (5057, 6736)\t0.07792895889266109\n",
      "  (5057, 7479)\t0.09172560005246519\n",
      "  (5057, 5315)\t0.18661142882710996\n",
      "  (5057, 7129)\t0.08405418557876088\n",
      "  (5057, 2355)\t0.09526504589303722\n",
      "  (5057, 6818)\t0.09612016746667819\n",
      "  (5057, 6875)\t0.06283120443838189\n",
      "  (5057, 6592)\t0.0829299089181746\n",
      "  (5057, 4522)\t0.10447990186450447\n",
      "  (5057, 7390)\t0.06457904852193093\n",
      "  (5057, 8640)\t0.08057222505655501\n",
      "  (5057, 9413)\t0.045392518727011825\n",
      "  (5057, 2368)\t0.05643001070864587\n",
      "  (5057, 6247)\t0.05317153322950402\n",
      "  (5057, 7529)\t0.09831585793976498\n",
      "  (5057, 6747)\t0.12376166904323836\n",
      "  (5057, 6479)\t0.09976327581680598\n",
      "  (5057, 6693)\t0.07274613332221683\n",
      "  (5057, 6434)\t0.09822974949503298\n",
      "  (5057, 8287)\t0.08537271877926758\n",
      "  (5057, 1776)\t0.053071427748035\n",
      "  (5057, 2146)\t0.03208411488068194\n"
     ]
    }
   ],
   "source": [
    "#initializing TfidfVectorizer with desired parameters\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#fitting and transforming the sample data\n",
    "term_doc_matrix = vectorizer.fit_transform(df_filter_2009['processed_text'])\n",
    "\n",
    "print(term_doc_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 295)\t0.12488781127211442\n",
      "  (0, 102)\t0.09345157065210982\n",
      "  (0, 166)\t0.07917429316932827\n",
      "  (0, 176)\t0.10760880020258352\n",
      "  (0, 170)\t0.07192251769831076\n",
      "  (0, 198)\t0.18425206890063509\n",
      "  (0, 165)\t0.09288969051104663\n",
      "  (0, 2)\t0.1874710652268045\n",
      "  (0, 100)\t0.12523935004952594\n",
      "  (0, 112)\t0.0704371851014994\n",
      "  (0, 13)\t0.16594201227300545\n",
      "  (0, 95)\t0.1682018076886719\n",
      "  (0, 134)\t0.11143038991206212\n",
      "  (0, 36)\t0.11728956032803683\n",
      "  (0, 133)\t0.13727421676613638\n",
      "  (0, 8)\t0.34409682091262905\n",
      "  (0, 225)\t0.374942130453609\n",
      "  (0, 79)\t0.2123079839948774\n",
      "  (0, 286)\t0.09756058409266898\n",
      "  (0, 247)\t0.06587819288445902\n",
      "  (0, 173)\t0.07230043515021628\n",
      "  (0, 290)\t0.13622799325800872\n",
      "  (0, 89)\t0.12488781127211442\n",
      "  (0, 81)\t0.2560860543936221\n",
      "  (0, 57)\t0.21802127755245213\n",
      "  :\t:\n",
      "  (199, 298)\t0.14280619547976742\n",
      "  (199, 26)\t0.07970491216418446\n",
      "  (199, 46)\t0.06976033618557186\n",
      "  (199, 130)\t0.10878690069127558\n",
      "  (199, 293)\t0.0802175947214536\n",
      "  (199, 125)\t0.17026979085552002\n",
      "  (199, 193)\t0.12879950319126698\n",
      "  (199, 126)\t0.059762085335405016\n",
      "  (199, 287)\t0.05881864563045274\n",
      "  (199, 269)\t0.06272564846745589\n",
      "  (199, 251)\t0.10851557940812238\n",
      "  (199, 295)\t0.3979291737943591\n",
      "  (199, 165)\t0.07399344544227733\n",
      "  (199, 100)\t0.04988115992276792\n",
      "  (199, 112)\t0.22443351826198957\n",
      "  (199, 36)\t0.09342972977231748\n",
      "  (199, 133)\t0.10934897309953387\n",
      "  (199, 173)\t0.11518518953627548\n",
      "  (199, 89)\t0.09948229344858978\n",
      "  (199, 81)\t0.20399130829323978\n",
      "  (199, 241)\t0.04489943301495892\n",
      "  (199, 32)\t0.45210387693448106\n",
      "  (199, 20)\t0.1367285404266383\n",
      "  (199, 14)\t0.08287858486468094\n",
      "  (199, 252)\t0.13537144321948683\n"
     ]
    }
   ],
   "source": [
    "#getting sample data from original data for checking the code\n",
    "sample_data = df_filter_2009[\"text\"][:200]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "\n",
    "term_doc_matrix = vectorizer.fit_transform(sample_data)\n",
    "print(term_doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part 3 - Computing SVD and Implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of U: (200, 200)\n",
      "Shape of Sigma: (200, 200)\n",
      "Shape of V_T: (300, 300)\n"
     ]
    }
   ],
   "source": [
    "#custom SVD function\n",
    "def svd(A):\n",
    "    Ui = A.dot(A.transpose())\n",
    "    Vi = A.transpose().dot(A)\n",
    "    _,eig_values_U, U = custom_eig(Ui)\n",
    "    U = U.real\n",
    "    eig_values_U.real\n",
    "    _,eig_values_V, V = custom_eig(Vi)\n",
    "    V = V.real\n",
    "    eig_values_V.real\n",
    "    # Sorting eigenvalues and corresponding eigenvectors in descending order\n",
    "    idx_U = np.argsort(eig_values_U)[::-1]\n",
    "    idx_V = np.argsort(eig_values_V)[::-1]\n",
    "    U = U[:, idx_U]\n",
    "    V = V[:, idx_V]\n",
    "    # Taking square root of positive eigenvalues\n",
    "    Si = np.sqrt(np.maximum(eig_values_U, 0))\n",
    "    S = np.diag(Si)\n",
    "    return U, S, V.transpose()\n",
    "\n",
    "#custom eigenvalue decomposition function\n",
    "def custom_eig(matrix, epsilon=1e-10, max_iterations=1000):\n",
    "    matrix_dense = matrix.toarray()  # Convert to dense array\n",
    "    m, n = matrix_dense.shape\n",
    "    eigenvalues = np.zeros(n)\n",
    "    eigenvectors = np.eye(n)\n",
    "    U = np.eye(m)  # Initialize U as a 2D identity matrix\n",
    "    for i in range(n):\n",
    "        v = np.random.rand(n)\n",
    "        for _ in range(max_iterations):\n",
    "            v_next = np.dot(matrix_dense, v)\n",
    "            v_next_norm = np.linalg.norm(v_next)\n",
    "            v_next /= v_next_norm\n",
    "            eigenvalue = np.dot(v_next, np.dot(matrix_dense, v_next))\n",
    "            if np.abs(eigenvalue - eigenvalues[i]) < epsilon:\n",
    "                break\n",
    "            v = v_next\n",
    "            eigenvalues[i] = eigenvalue\n",
    "        eigenvectors[:, i] = v_next\n",
    "        U[:, i] = v  # Update U with the computed eigenvector\n",
    "        matrix_dense -= eigenvalues[i] * np.outer(v_next, v_next)\n",
    "    return eigenvectors, eigenvalues, U\n",
    "\n",
    "#computing SVD \n",
    "U, Sigma, V_T = svd(term_doc_matrix)\n",
    "\n",
    "#printing shapes of resulting matrices\n",
    "print(\"Shape of U:\", U.shape)\n",
    "print(\"Shape of Sigma:\", Sigma.shape)\n",
    "print(\"Shape of V_T:\", V_T.shape)\n",
    "\n",
    "#evaluating SVD Approximation\n",
    "def calculate_mse(original_matrix, reconstructed_matrix):\n",
    "    #MSE\n",
    "    mse = np.mean(np.square(original_matrix - reconstructed_matrix))\n",
    "    return mse\n",
    "\n",
    "def calculate_frobenius_norm(original_matrix, reconstructed_matrix):\n",
    "    #frobenius norm\n",
    "    fn = np.linalg.norm(original_matrix - reconstructed_matrix, ord='fro')\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 200\n",
      "d: 300\n",
      "21\n",
      "41\n",
      "61\n",
      "81\n",
      "101\n",
      "121\n",
      "141\n",
      "161\n",
      "181\n",
      "k_values: [21, 41, 61, 81, 101, 121, 141, 161, 181]\n",
      "k: 21\n",
      "MSE: 0.04907505524265239\n",
      "Frobenius Norm: 54.26327777198078\n",
      "==================================================\n",
      "k: 41\n",
      "MSE: 0.06301750588247766\n",
      "Frobenius Norm: 61.49024599843995\n",
      "==================================================\n",
      "k: 61\n",
      "MSE: 0.07275902377833385\n",
      "Frobenius Norm: 66.07224399625026\n",
      "==================================================\n",
      "k: 81\n",
      "MSE: 0.08850652790041792\n",
      "Frobenius Norm: 72.87243425346155\n",
      "==================================================\n",
      "k: 101\n",
      "MSE: 0.09063798802609442\n",
      "Frobenius Norm: 73.74468985334242\n",
      "==================================================\n",
      "k: 121\n",
      "MSE: 0.09282712063094313\n",
      "Frobenius Norm: 74.62993526632987\n",
      "==================================================\n",
      "k: 141\n",
      "MSE: 0.08987034846763518\n",
      "Frobenius Norm: 73.43174319092603\n",
      "==================================================\n",
      "k: 161\n",
      "MSE: 0.09955969031462826\n",
      "Frobenius Norm: 77.28894758552283\n",
      "==================================================\n",
      "k: 181\n",
      "MSE: 0.09193111256014257\n",
      "Frobenius Norm: 74.26888146194578\n",
      "==================================================\n",
      "Length of mse_values: 9\n",
      "Length of fn_values: 9\n",
      "Optimal k for MSE: 21\n",
      "Optimal k for Frobenius Norm: 21\n"
     ]
    }
   ],
   "source": [
    "#printing dimensions of term_doc_matrix\n",
    "t, d = term_doc_matrix.shape\n",
    "print(\"t:\", t)\n",
    "print(\"d:\", d)\n",
    "\n",
    "min_k = max(10, min(t, d) // 10 + 1)        #determining minimum value of k (number of singular values to keep)\n",
    "k_values = []                               #generating a list of k values to iterate over\n",
    "for i in range(min_k, min(t, d) + 1, 20):\n",
    "    print(i)\n",
    "    k_values.append(i)\n",
    "\n",
    "print(\"k_values:\", list(k_values))\n",
    "\n",
    "mse_values = []\n",
    "fn_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    U_k, Sigma_k, V_T_k = svd(term_doc_matrix)                  #performing SVD on term_doc_matrix\n",
    "    U_k = U_k[:, :k]\n",
    "    Sigma_k = Sigma_k[:k, :k]                                   #keeping only first k singular values\n",
    "    V_T_k = V_T_k[:, :k]\n",
    "    reconstructed_matrix = np.dot(np.dot(U_k, Sigma_k), V_T_k.transpose())      #reconstructing the matrix using the truncated SVD\n",
    "\n",
    "    #calculating Mean Squared Error (MSE) and Frobenius Norm (FN)\n",
    "    mse = calculate_mse(term_doc_matrix, reconstructed_matrix)  \n",
    "    fn = calculate_frobenius_norm(term_doc_matrix, reconstructed_matrix)  \n",
    "    mse_values.append(mse)\n",
    "    fn_values.append(fn)\n",
    "    \n",
    "    print(\"k:\", k)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"Frobenius Norm:\", fn)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "#checking if MSE and FN lists are empty   \n",
    "if not mse_values:\n",
    "    print(\"Error: mse_values is empty.\")\n",
    "if not fn_values:\n",
    "    print(\"Error: fn_values is empty.\")\n",
    "\n",
    "print(\"Length of mse_values:\", len(mse_values))\n",
    "print(\"Length of fn_values:\", len(fn_values))\n",
    "\n",
    "#finding optimal k values for MSE and FN\n",
    "optimal_k_mse = k_values[np.argmin(mse_values)]\n",
    "optimal_k_fn = k_values[np.argmin(fn_values)]\n",
    "\n",
    "print(\"Optimal k for MSE:\", optimal_k_mse)\n",
    "print(\"Optimal k for Frobenius Norm:\", optimal_k_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part 4 - Query-Document Cosine Similarity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant document for query 1:\n",
      "Text: I used to love Comcast. Until all these constant updates. My internet and cable crash a lot at night, and sometimes during the day, some channels don't even work and on demand sometimes don't play either. I wish they will do something about it. Because just a few mins ago, the internet have crashed for about 20 mins for no reason. I'm tired of it and thinking about switching to Wow or something. Please do not get Xfinity.\n",
      "Cosine Similarity: nan\n",
      "==================================================\n",
      "Most relevant document for query 2:\n",
      "Text: I called Xfinity to troubleshoot my internet at 4 am 2/17/2016, got connected to someone who I swear came to this country on a floating door as I had to tell him twice I was outside and not near the router, had to tell him three times I was NOT going to set an appointment for a service technician as I wasn't going to pay $25 for that.He tried resetting my router which did not work, and I eventually hung up on him, went on my phone to xfinity chat and was told again that there was no server maintenance (I don't believe that) but was told my router/modem through xfinity had device problems, got ready to set up an appointment for 2/17/2016 and as soon as was in the process of setting up the appointment my internet magically started working. It seems to me that the customer service technicians either don't know when/if there is server maintenance or that xfinity is such a money hungry company that they shut off internet just to get you to schedule a service technician to come out so they can rape you of more money.\n",
      "Cosine Similarity: 0.3388979012226989\n",
      "==================================================\n",
      "Most relevant document for query 3:\n",
      "Text: I'm so over Comcast! The worst internet provider. I'm taking online classes and multiple times was late with my assignments because of the power interruptions in my area that lead to poor quality internet service. Definitely switching to Verizon. I'd rather pay $10 extra then dealing w/ Comcast and non stopping internet problems.\n",
      "Cosine Similarity: 0.36520337415709064\n",
      "==================================================\n",
      "Most relevant document for query 4:\n",
      "Text: I'm so over Comcast! The worst internet provider. I'm taking online classes and multiple times was late with my assignments because of the power interruptions in my area that lead to poor quality internet service. Definitely switching to Verizon. I'd rather pay $10 extra then dealing w/ Comcast and non stopping internet problems.\n",
      "Cosine Similarity: 0.36520337415709064\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-652853e58c16>:17: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  sim.append(np.dot(query, doc) / (np.linalg.norm(query) * np.linalg.norm(doc)))\n"
     ]
    }
   ],
   "source": [
    "#query-document cosine similarity\n",
    "queries = [\n",
    "    ['ignorant', 'overwhelming'],\n",
    "    ['xfinity', 'frustrate', 'adapter', 'verizon', 'router'],\n",
    "    ['terminate', 'rent', 'promotion', 'joke', 'liar', 'internet', 'horrible'],\n",
    "    ['kindergarten', 'ridiculous', 'internet', 'clerk', 'terrible']\n",
    "]\n",
    "\n",
    "#calculating TF-IDF matrix for queries\n",
    "query_matrix = vectorizer.transform([' '.join(query) for query in queries]).toarray()\n",
    "\n",
    "#calculating cosine similarity \n",
    "cosine_similarities = []\n",
    "for query in query_matrix:\n",
    "    sim = []\n",
    "    for doc in term_doc_matrix.toarray():\n",
    "        sim.append(np.dot(query, doc) / (np.linalg.norm(query) * np.linalg.norm(doc)))\n",
    "    cosine_similarities.append(sim)\n",
    "\n",
    "cosine_similarities = np.array(cosine_similarities)\n",
    "\n",
    "#finding the most relevant document for each query\n",
    "for i, query in enumerate(queries):\n",
    "    most_similar_doc_index = np.argmax(cosine_similarities[i])\n",
    "    most_similar_doc_text = df_filter_2009.iloc[most_similar_doc_index]['text']\n",
    "    print(f\"Most relevant document for query {i+1}:\")\n",
    "    print(\"Text:\", most_similar_doc_text)\n",
    "    print(\"Cosine Similarity:\", cosine_similarities[i, most_similar_doc_index])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
